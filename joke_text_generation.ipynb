{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "joke-text-generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQz0PU16IrdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "0262c86c-421a-4c7d-a5a5-9f6624f3cfcc"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-23 15:12:18--  https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 141847 (139K) [text/plain]\n",
            "Saving to: ‘reddit-cleanjokes.csv.2’\n",
            "\n",
            "reddit-cleanjokes.c 100%[===================>] 138.52K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-09-23 15:12:18 (5.65 MB/s) - ‘reddit-cleanjokes.csv.2’ saved [141847/141847]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1zbV1X1I2MI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ug6HWdDJezl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self,dataset):\n",
        "    super(Model,self).__init__()\n",
        "    self.lstm_size = 128\n",
        "    self.embedding_dim = 128\n",
        "    self.num_layers = 3\n",
        "  \n",
        "    n_vocab = len(dataset.uniq_words)\n",
        "    self.embedding = nn.Embedding(\n",
        "        num_embeddings = n_vocab,\n",
        "        embedding_dim=self.embedding_dim\n",
        "    )\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size = self.lstm_size,\n",
        "        hidden_size = self.lstm_size,\n",
        "        num_layers = self.num_layers,\n",
        "        dropout = 0.2\n",
        "    )\n",
        "    self.fc = nn.Linear(self.lstm_size, n_vocab).cuda()\n",
        "\n",
        "  def forward(self, x , prev_state):\n",
        "    embed = self.embedding(x)\n",
        "    output, state = self.lstm(embed, prev_state)\n",
        "    logits = self.fc(output)\n",
        "    return logits, state\n",
        "\n",
        "  def init_state(self, seq_length):\n",
        "    return (\n",
        "        torch.zeros(self.num_layers, seq_length, self.lstm_size,device=device),\n",
        "        torch.zeros(self.num_layers, seq_length, self.lstm_size,device=device)\n",
        "    )"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mXhxM1cK5pQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from collections import Counter"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ODivGJaN_7C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d5065870-db51-48ed-c3e4-c3d6f7a02f0d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reddit-cleanjokes.csv\t reddit-cleanjokes.csv.2\n",
            "reddit-cleanjokes.csv.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUz0Bo2UM4uO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,args):\n",
        "    self.args = args\n",
        "    self.words = self.load_words()\n",
        "    self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "    self.index_to_word = {index: word for index,word in enumerate(self.uniq_words)}\n",
        "    self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "    self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "  def load_words(self):\n",
        "    train_df = pd.read_csv('reddit-cleanjokes.csv')\n",
        "    text = train_df['Joke'].str.cat(sep=' ')\n",
        "    return text.split(\" \")\n",
        "  \n",
        "  def get_uniq_words(self):\n",
        "    word_counts = Counter(self.words)\n",
        "    return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return(\n",
        "        torch.tensor(self.words_indexes[index:index+self.args.sequence_length],device=device),\n",
        "        torch.tensor(self.words_indexes[index+1: index+self.args.sequence_length+1],device=device)\n",
        "    )\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U16Mx93fPPZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from torch import optim\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Wg3KVAS0V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dt, model, args):\n",
        "  model.train()\n",
        "  dataloader = DataLoader(dt, batch_size=args.batch_size)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  print_step = 0\n",
        "  for epoch in range(args.max_epochs):\n",
        "    state_h, state_c = model.init_state(args.sequence_length)\n",
        "\n",
        "    for batch, (x,y) in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "      y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "      loss = criterion(y_pred.transpose(1,2),y)\n",
        "\n",
        "      state_h = state_h.detach()\n",
        "      state_c = state_c.detach()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      print_step +=1\n",
        "      if print_step % 100 == 0:\n",
        "        print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBja17IcUDYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(dt, model, text,next_words = 100):\n",
        "  model.eval()\n",
        "  words = text.split(\" \")\n",
        "  state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "  for i in range(0, next_words):\n",
        "    x = torch.tensor([[dt.word_to_index[w] for w in words[i:]]],device=device)\n",
        "    y_pred,(state_h, state_c) = model(x, (state_h, state_c))\n",
        "    last_word_logits = y_pred[0][-1]\n",
        "    p = torch.nn.functional.softmax(last_word_logits,dim=0).cpu().detach().numpy()\n",
        "    word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "    words.append(dt.index_to_word[word_index])\n",
        "  return words"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeLfmCfOb9Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class aargs:\n",
        "  def __init__(self):\n",
        "    super(aargs, self).__init__()\n",
        "    self.batch_size = 256\n",
        "    self.sequence_length = 4\n",
        "    self.max_epochs = 50"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y2WX9_PZGNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = Dataset(aargs())\n",
        "model = Model(dataset).to(device)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XFTZNUQfRxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "outputId": "3aece4a8-360d-4312-84b4-7b6c54ff4fba"
      },
      "source": [
        "train(dataset, model, aargs())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'batch': 5, 'loss': 4.678015232086182}\n",
            "{'epoch': 2, 'batch': 11, 'loss': 4.471127033233643}\n",
            "{'epoch': 3, 'batch': 17, 'loss': 4.442663669586182}\n",
            "{'epoch': 4, 'batch': 23, 'loss': 4.640810012817383}\n",
            "{'epoch': 5, 'batch': 29, 'loss': 4.648361682891846}\n",
            "{'epoch': 6, 'batch': 35, 'loss': 4.33594274520874}\n",
            "{'epoch': 7, 'batch': 41, 'loss': 4.023390293121338}\n",
            "{'epoch': 8, 'batch': 47, 'loss': 4.4371562004089355}\n",
            "{'epoch': 9, 'batch': 53, 'loss': 3.9657838344573975}\n",
            "{'epoch': 10, 'batch': 59, 'loss': 3.821251630783081}\n",
            "{'epoch': 11, 'batch': 65, 'loss': 3.6824791431427}\n",
            "{'epoch': 12, 'batch': 71, 'loss': 3.744523763656616}\n",
            "{'epoch': 13, 'batch': 77, 'loss': 3.7176854610443115}\n",
            "{'epoch': 14, 'batch': 83, 'loss': 3.6293230056762695}\n",
            "{'epoch': 15, 'batch': 89, 'loss': 3.539944887161255}\n",
            "{'epoch': 17, 'batch': 1, 'loss': 3.2996034622192383}\n",
            "{'epoch': 18, 'batch': 7, 'loss': 3.52400541305542}\n",
            "{'epoch': 19, 'batch': 13, 'loss': 3.4713447093963623}\n",
            "{'epoch': 20, 'batch': 19, 'loss': 3.371743679046631}\n",
            "{'epoch': 21, 'batch': 25, 'loss': 3.279231548309326}\n",
            "{'epoch': 22, 'batch': 31, 'loss': 2.65604305267334}\n",
            "{'epoch': 23, 'batch': 37, 'loss': 2.976651430130005}\n",
            "{'epoch': 24, 'batch': 43, 'loss': 2.9637105464935303}\n",
            "{'epoch': 25, 'batch': 49, 'loss': 2.863579273223877}\n",
            "{'epoch': 26, 'batch': 55, 'loss': 2.6421430110931396}\n",
            "{'epoch': 27, 'batch': 61, 'loss': 2.670598030090332}\n",
            "{'epoch': 28, 'batch': 67, 'loss': 2.498342752456665}\n",
            "{'epoch': 29, 'batch': 73, 'loss': 2.6389122009277344}\n",
            "{'epoch': 30, 'batch': 79, 'loss': 2.27677845954895}\n",
            "{'epoch': 31, 'batch': 85, 'loss': 2.4488422870635986}\n",
            "{'epoch': 32, 'batch': 91, 'loss': 2.32881498336792}\n",
            "{'epoch': 34, 'batch': 3, 'loss': 2.462960720062256}\n",
            "{'epoch': 35, 'batch': 9, 'loss': 2.5130696296691895}\n",
            "{'epoch': 36, 'batch': 15, 'loss': 2.240358829498291}\n",
            "{'epoch': 37, 'batch': 21, 'loss': 2.1237668991088867}\n",
            "{'epoch': 38, 'batch': 27, 'loss': 2.2087810039520264}\n",
            "{'epoch': 39, 'batch': 33, 'loss': 2.1728053092956543}\n",
            "{'epoch': 40, 'batch': 39, 'loss': 2.050966739654541}\n",
            "{'epoch': 41, 'batch': 45, 'loss': 2.248293161392212}\n",
            "{'epoch': 42, 'batch': 51, 'loss': 2.2483601570129395}\n",
            "{'epoch': 43, 'batch': 57, 'loss': 2.1562275886535645}\n",
            "{'epoch': 44, 'batch': 63, 'loss': 1.9957383871078491}\n",
            "{'epoch': 45, 'batch': 69, 'loss': 1.8974136114120483}\n",
            "{'epoch': 46, 'batch': 75, 'loss': 2.023420572280884}\n",
            "{'epoch': 47, 'batch': 81, 'loss': 2.086909770965576}\n",
            "{'epoch': 48, 'batch': 87, 'loss': 2.0129237174987793}\n",
            "{'epoch': 49, 'batch': 93, 'loss': 1.2692465782165527}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdILjQO_fXuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0fbb37a6-6c2b-4aa8-9e39-37167ea83978"
      },
      "source": [
        "joke = predict(dataset, model, text='Knock knock. Whos there?')\n",
        "print(' '.join(joke))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Knock knock. Whos there? scientist 7 mysterious do celebrity hear much coffee banana night? Dayton go? well. Why did the alphabet try his Holland? mistake. depressed? Because of art fun out What did the grape say to the lid You Mr. start. Because he was my Service. classical single that's my sitting cereal... A Clown in the seeing-eye reason. I don't have them down. Why did the egg cross the shark from the great deer A orphans? Hose pork Why did the Bicycle get a picture? V Sinatra\" Because she had in salt check of paper. What did the Gogh say to the mass\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}